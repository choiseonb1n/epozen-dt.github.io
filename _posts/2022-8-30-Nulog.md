---
title: "NuLog"
last_modified_at: 2022-8-30
author: Gun-ha, KANG
---

> 이번 포스팅에서는 **DL 기반의 로그 파싱 기술인 NuLog**에 대해 공유해보려고 합니다.

## **NuLog**

> **제안**   
* 트랜스포머 구조를 활용하는 NuLog라는 새로운 로그 파싱 기술 제안합니다.
* MLM 과 self-supervised Learning(pre-trained Model 생성과 Downstream Task 즉, 전이학습하겠다는 의미)

> **비교**  
* 다른 12개의 로그 파싱 알고리즘과 비교해보았을 때, 전반적으로 높은 성능을 보인다고 합니다.

![1](https://user-images.githubusercontent.com/92897860/187346230-9c24aa4e-94ac-41b7-a375-0e9db295b25d.png)

이에 본 포스팅에서는 이에 대한 논문 내용을 정리하며, LogHub에서 제공하는 로그 데이터를 적용해보았습니다.

### **서론**

생성된 벡터 표현은 동일한 로그 템플릿에 속하는 로그 메시지의 경우 더 가까운 임베딩 벡터여야 하며, 별개의 로그 템플릿에 속하는 로그 메시지의 경우 멀리 있는 임베딩 벡터여야 합니다.   
예를 들어 "Took 10 seconds to create a VM" 및 "Took 9 seconds to create a VM"의 내장 벡터는 거리가 작아야 하며 "Took 9 seconds to create a VM" 및 "Failed to create VM 3"의 벡터는 거리가 멀어야 합니다.


### **정의 및 구성**

제안된 방법은 전처리 과정과 모델 및 로그 템플릿 추출 단계로 이루어집니다.  
이러한 과정을 통해 로그의 일반적인 semantic representation 을 학습합니다.

![2](https://user-images.githubusercontent.com/92897860/187347306-70748700-fe0a-49af-9117-a2d2756863fc.png)


> **Tokenization** 
* 문자열을 토큰으로 나누는 전처리 과정입니다. 
* 보통 형태소, 어절 단위로 문자열을 나눕니다.

> **Masking** 
* MLM 을 하기 위해서 Masking된 형식으로 변환하는 전처리 과정입니다. 
* CLS, MASK, SPEC 토큰으로 변환한다는 의미로 해석할 수 있습니다.

> **Model** 
* 이 모델의 입력은 Token Embeddings와 positional Encoding의 두 가지 연산을 적용합니다. 이는 토큰을 토큰화하고, 각 토큰의 위치를 식별하기 위한 연산입니다. (*Attention 기반의 모델은 RNN과 달리 입력 순서의 개념을 포함하지 않기 때문에 Positional Encoding이 필요합니다.)
* 구성으로는 Multi-head Attention 과 Residual Connection 및 2개의 Feed Forward Network로 구성되어 있습니다. 또한, Generator를 통해 이벤트 템플릿 추출을 목표로 합니다. 즉, 트랜스포머 인코더를 변형한 구조에 Softmax와 Generator를 통해 로그가 가지는 representation을 학습합니다.
* 이때 학습 시, MLM 방법론인 임의로 input token 일부를 masking하고, objective는 context를 통해 이러한 masked token을 예측하는 방식을 사용합니다.

![3](https://user-images.githubusercontent.com/92897860/187351978-80f24480-48fb-4dea-9dc2-54f30e47967a.png)

※ _기본적인 모델의 메커니즘은 Transformer 논문을 선행해야 합니다._

> **Log Template Extraction** 
* 로그 데이터셋 내의 모든 로그 템플릿 추출은 모델 학습 후 온라인 모드로 실행되며, 이는 각 토큰을 예측하는 모델의 능력을 측정하여 토큰이 템플릿이 상수 부분인지 변수인지 결정합니다.
* 로그에서 모든 이벤트 템플릿을 식별하는 작업을 고려할 때, 지속적으로 다시 나타나는 부분에 세심한 주의를 기울이고 특정 컨텍스트 내에서 자주 변경되는 부분은 무시합니다.


### **MLM**

Cloze Test에서 영감을 받았으며, 빈칸 뚫고 맞추는 학습 방식이라고 생각하면 됩니다.  


일련의 단어가 주어지면 입력으로 무작위하게 몇 개의 토큰을 마스킹하고 모델에 넣어 주변 단어의 맥락으로 마스킹된 토큰을 예측합니다.

세부적으로는, 입력의 15%에 토큰을 부여하고 그 15% 중 80%는 [MASK] 토큰을 10%는 원래 단어를 나머지 10%는 랜덤한 단어를 부여합니다. 이는 특정 단어의 representation을 학습하기 위한 방식이며, 사전학습에서만 MLM학습을 사용하기 때문에 사전학습과 파인튜닝간의 mismatch를 해소하기 위해 다음과 같이 사용합니다. BERT에서는 Wordpiece 단어 중 [MASK]에 들어갈 단어를 찾았는데 본 논문에서는 특별한 말은 없었습니다.

![4](https://user-images.githubusercontent.com/92897860/187354591-951c6524-0b5e-4da5-9ba1-6dc6fd909308.png)

※ _기본적인 MLM의 메커니즘은 BERT 논문을 선행해야 합니다._

### **테스트**

LogHub에서 제공하는 Spark 로그 데이터를 사용했습니다.

> **테스트 결과**

로그 템플릿 추출 결과로, 생각보다 많은 부분을 변수로 처리한 모습을 볼 수 있습니다.

![5](https://user-images.githubusercontent.com/92897860/187356942-3e577725-6563-4539-b40b-6cb838e8f60f.png)

속도 확인 결과, 2000건은 16.87초가 걸렸으며, 이는 1건 당 0.008초를 의미한다.

![6](https://user-images.githubusercontent.com/92897860/187356946-ba49db51-758f-484e-884f-a6f03e440f57.png)


### **정리 및 결론**

출력된 결과로 봤을 때는 원하는 형태까지의 Structure한 로그 파싱은 아니었습니다.
다만, NLP 언어모델과 같이 (로그)데이터가 가지는 Representation을 학습하려고 한다는 점과 이후 로그 이상탐지와 같은 Task를 Fine-tuning할 수 있다는 점에서 좀 더 연구를 해봐야할 것같습니다.


> **참고 논문 및 자료**  

* [Self-Supervised Log Parsing](https://arxiv.org/abs/1810.03548)  
* [Self-Supervised Log Parsing-official-code(Pytorch)](https://github.com/nulog/nulog)  
* [BERT: Pre-trainig of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)  
* ["Cloze Procedure": A New Tool For Measuring Readability](https://www.gwern.net/docs/psychology/writing/1953-taylor.pdf)  
* [LogHub](https://github.com/logpai/loghub)  


---

<details>
  <summary><b>Contact</b></summary>

<b>Author. </b>KangGunha

<b>Email. </b>zxcvbnm9931@epozen.com

</details>
