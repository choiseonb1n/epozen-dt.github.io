---
title: "자동 로그 파싱 - Drain"
last_modified_at: 2022-7-21
author: 이한솔
---

이 포스팅에서는 자동 로그 파싱 알고리즘인 Drain에 대해 알아보겠습니다.

---

# **목차**
1. Drain
2. Drain의 목표
3. Drain 알고리즘
4. Drain3
5. 테스트

---

# **Drain**

Drain은 자동 로그 파싱 알고리즘이며 Offline, Online에서 모두 가능한 파싱 방법입니다.

전통적 로그 파싱의 문제점은 정규식에 크게 의존한다는 점인데, 이는 로그 양이 방대하고 오픈 소스 플랫폼이 즐비한 현대 서비스에서 적합하지 않아 Drain이 제안되었습니다.

특징으로는 **Fixed depth parse tree**를 사용하고 있습니다.

---

# **Drain의 목표**
Drain의 목표는 구조화된 로그 메시지로 변환하는 것입니다.

<img src="https://user-images.githubusercontent.com/96156882/180102818-eb8488b0-e415-48ad-b95a-42710886076e.png" width="500">

먼저 상수 부분과 변수 부분을 구분합니다.

상수 부분은 로그 이벤트, 변수 부분은 동적 런타임 시스템 정보를 전달하는 나머지 토큰입니다.

> **상수(로그 이벤트)가 동일한 로그 이벤트를 로그 그룹으로 클러스터링** 하는 것이 Drain의 목표라고 할 수 있습니다.

---

# **Drain 알고리즘**
Drain은 Fixed depth parse tree를 사용한다는 특징을 가지고 있습니다.

Drain 알고리즘은 다음과 같습니다.

<img src="https://user-images.githubusercontent.com/96156882/180103256-2e4dd063-dc9d-47f2-ad54-6c3f55ff6e32.png" width="500">

1. 간단한 정규식을 이용해 전처리

    ip, 숫자 등 간단하게 정규식을 이용해 전처리를 합니다.
    이로 인해 1차적으로 상수와 변수를 구분할 수 있고, 무분별한 로그 그룹 생성을 막을 수 있습니다.

2. 로그 메시지 길이(토큰)로 검색
    
    예를 들어 로그가 "Receive from node 4"일 때 4개의 토큰으로 나뉘어 Length 4 노드로 이동하게 됩니다.
    
3. 선행 토큰으로 검색

    그 다음 선행 토큰인 Receive로 이동하게 됩니다. 그림에서는 Depth가 3이므로 첫번째 토큰만 검색합니다.

4. 적합한 로그 그룹 검색
   
    로그 메시지와 각 로그 그룹의 이벤트 간 유사성을 계산합니다.
    유사성 식은 다음과 같습니다.

    <img src="https://user-images.githubusercontent.com/96156882/180103684-24122ba2-93ae-47f1-8e3a-03b71c2e601b.png" width="400">

5. 트리 업데이트
   
    4단계에서 검색된 로그 그룹에 현재 로그 메시지의 로그 ID를 추가합니다. 만약 새로운 노드가 생성됐다면 트리를 업데이트 합니다.
    
    <img src="https://user-images.githubusercontent.com/96156882/180103785-1da0c376-bc1c-473d-ad28-862562323ad1.png" width="500">

---
# **Drain3**
Drain3는 실제 사용을 위해 Python3에 맞게 조정하여 Drain을 확장한 오픈 소스입니다.

실제로 IBM에서 네트워크 중단 모니터링을 위해 사용하였으며 Masking, Persistence를 추가하고 리팩토링 및 버그 수정을 통해 보다 유용하게 Drain을 적용할 수 있습니다.

\[오픈 소스] : https://github.com/IBM/Drain3 

---

# **테스트**

간단하게 Drain3를 이용해 테스트를 진행해보았습니다.

로그 파일은 loghub의 HDFS_2k.log 파일을 이용하였습니다.
설정은 전처리(Ip, Num), 유사성 임계값 0.5, 깊이 4로 진행하였습니다.

1. 구조화된 부분을 제거하였습니다.
    
    타임 스탬프, 호스트 이름 등 구조화된 부분을 제거할 경우 템플릿 마이닝 정확도가 향상될 수 있습니다. 저는 간단하게 타임 스탬프만 삭제 후 테스트 했습니다.
    ![image](https://user-images.githubusercontent.com/96156882/180104529-50b168fc-feef-474f-9ec3-b3e3227f7816.png)
    
2. 먼저 Offline 파싱을 이용해, 로그 그룹 생성 후 bin 파일로 저장하였습니다.
   
    2000개의 로그 샘플로 16개의 로그 그룹이 생성되었습니다.
    ![image](https://user-images.githubusercontent.com/96156882/180104617-ee1131e5-6fb4-4695-bb2a-b83cb8bb7833.png)

3. 다음은 Online 테스트를 진행하였습니다.

    - 로그 입력시 어떤 로그 그룹에 해당하는지 검색 후 변수 부분(Parameters)을 추출하는 것을 볼 수 있습니다.
    ![image](https://user-images.githubusercontent.com/96156882/180104847-c9cf9661-a40f-49c2-a6f2-7167cae4be32.png)
    - 새로운 로그 그룹 생성시 트리가 업데이트 됩니다.
    ![image](https://user-images.githubusercontent.com/96156882/180104885-e2e527db-255c-4245-a40c-c0b0580b2d25.png)

---

이번 포스팅에서는 자동 로그 파싱인 Drain에 대해 알아보았습니다.
성능면에서도 높은 정확도와 빠른 속도를 보이고 있고, IBM에서 사용하고 있어 꾸준히 성능 개선 업데이트 될 확률이 높기 때문에 Online 파싱 이용에 적합한 파싱 방법 같습니다.

---

## 참고자료
- Pinjia He, Jieming Zhu, Zibin Zheng, and Michael R. Lyu. Drain: An Online Log Parsing Approach with Fixed Depth Tree, Proceedings of the 24th International Conference on Web Services (ICWS), 2017. 
- https://pypi.org/project/drain3/ 

