---
title: "GPT - 개요"
last_modified_at: 2022-10-31
author: 김수민
---

이번 포스팅에서는 언어모델 중 GPT(Generative Pretrained Transformer)에 대해 알아보도록 하겠습니다.


# GPT(Generative Pretrained Transformer)

GPT모델은 이름에서와 같이 <u>Transformer 기반의 사전 학습된 생성 모델</u>입니다.

기본적으로 다음 단어를 예측하는 언어모델(Language Model)의 한 종류로 분류됩니다.

------

### Transformer

Transformer는 기존 Seq2Seq모델의 인코더-디코더 구조를 갖는 Attention 모델입니다.

큰 특징으로는 RNN을 사용하지 않고 Attention만을 사용해 구조를 이루고 있다는 점 입니다.

다음은 Transformer의 구조입니다.

![selfa5](https://user-images.githubusercontent.com/70212461/187125087-b32b7f59-3d00-499e-8825-cf872d4b904e.png)

Transfomer 구조에 대한 설명은 다른 포스팅에서 자세히 다루고 있습니다.

https://epozen-dt.github.io/Transformer-Self-Attention/

------

### GPT 시리즈

GPT는 OpenAI에서 공개한 모델입니다. 

2018년부터 꾸준히 새로운 모델들을 만들어 발표하는데, GPT-1, GPT-2, GPT-3가 바로 이 모델들입니다.

이 세 모델의 기본적인 구조, 학습원리는 동일합니다.

다만, 세 모델의 가장 큰 차이점은 파라미터 사이즈들과 학습 데이터 양입니다.

GPT -1의 경우 논문으로 공개되었으나, GPT-2는 오픈소스로 모델 자체를 공개하고 있지는 않습니다. GPT-3의 경우 유료 API로 제공됩니다.

1. #### GPT1 → GPT2

   layer의 순서가 달라짐과 사용된 데이터의 용량이 매우 증가했습니다.

2. #### GPT2 → GPT3

   Few-Shot learning 이 가장 큰 특징 입니다.

   지금까지의 NLP모델들은 모두 Pre-training 후 Task에 따른 Finetuning을 진행했지만, GPT-3에서는 Finetuning을 진행하지 않고 task를 수행하는 것을 시도했습니다.

   이때 사용하는 Few shot learning은 말 그대로 적은 데이터로 데이터 특징을 파악해가는 방식입니다.

------

### GPT 구조

GPT는 앞서 설명한 바와 같이 Transformer로부터 기원된 모델 구조를 갖고 있습니다.

Transformer의 Decoder을 겹겹히 쌓은 구조로 다음과 같이 그릴 수 있습니다. 아래의 구조 중 첫 번째 그림은 transformer에서 GPT 구조의 차이를 보여주기 위한 그림입니다.

즉, transformer에서 Encoder구조와 Encoder에서 넘어오는 데이터를 사용하는 구조가 사라진 형태가 바로 두 번째 그림인 GPT-2의 구조입니다.

GPT 시리즈 별로 약간의 순서 차이가 있지만 큰 구조는 변하지 않습니다.

<center><img src="https://i.imgur.com/Q7IS78n.png" alt="source: imgur.com" style="zoom:40%;" />&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;<img src="https://blog.kakaocdn.net/dn/duvtAY/btq9JObJmZ3/wsPRgh1NISwJZxrNl4nmek/img.png" alt="img" style="zoom:50%;" />   </center>

각 레이어의 연산 방식 역시 transformer의 내용과 동일하니, 다음 포스팅에서 설명하는 각 레이어에 대한 설명을 참고하세요.

https://epozen-dt.github.io/Transformer_Encoder,Decoder/#decoder

---

참고로 input 형태의 경우, 적용 task에 맞게 입력 구조의 변환이 필요합니다.

이는 GPT 모델이 pre-trained 모델로서 원하는 task에 맞춰 구성되어져 있지 않기 때문입니다.

<img src="https://user-images.githubusercontent.com/54731898/137593951-4927546d-9138-4918-b520-bc3d3684973b.png" alt="image" style="zoom:80%;" />

이러한 supervised task에 데이터셋이 주어진 경우 해당 데이터셋에 대한 fine-tuning을 진행하는 것이 GPT모델의 학습이 됩니다.

---

이번 포스팅에서는 GPT에 대한 개요에 대하여 알아보았습니다.

요약해보자면 다음과 같습니다.

1. GPT는 transformer의 decoder 구조를 사용해 다음 단어를 예측하는 언어모델입니다.

2. GPT는 pre-trained 모델이며, 원하는 task에 맞춰 입력 구조를 바꾸어 적용하게 됩니다.




## 출처

- https://ratsgo.github.io/nlpbook/docs/language_model/bert_gpt/#gpt
- https://jiho-ml.com/weekly-nlp-29/
- https://yseon99.tistory.com/127
- https://velog.io/@jus6886/%EB%85%BC%EB%AC%B8%EB%A6%AC%EB%B7%B0-GPT1-GPT2-GPT3-%EC%B0%A8%EC%9D%B4%EC%99%80-%ED%95%9C%EA%B3%84
