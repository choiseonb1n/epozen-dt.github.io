---
title: "Sequence to Sequence 개요"
last_modified_at: 2022-11-30
author: 김수민
---

이번 포스팅은 Seq2Seq 모델에 대하여 알아보겠습니다.

---

>## Sequence to Sequence&nbsp;&nbsp;

Seq2Seq(Sequence to Sequence)는 시계열 데이터를 다른 시계열 데이터로 변환하는 기능을 갖는 모델입니다.

보통 RNN(Recurrent Neural Network) 또는 LSTM(Long Short-Term Memory)를 사용해 Encoder-Decoder 구조로 만들어 구성합니다.

이때, Encoder-Decoder 구조는,

1. 입력받은 순차데이터를 압축해 하나의 벡터(context vector) 생성 후,
2. 생성된 벡터(context vector)를 하나씩 순차적으로 출력하는 형태입니다.

![image](https://user-images.githubusercontent.com/87166420/204735051-34e25732-ccf7-499c-9860-6f9f0e2ec30f.png)

각 구조별로 어떻게 수행되는지에 대해 알아보겠습니다.

---

### 1. 인코더

![image](https://user-images.githubusercontent.com/87166420/204735675-8d51bb7e-692a-4160-a38e-2acd8016d794.png)

인코더에서는 번역하고자 하는 문장이 입력되어 context vector을 생성하는 단계입니다.



우선 이 과정에서 입력된 문장들은 단어 단위로 쪼개지고, 각 단어를 토큰화해 사용합니다.

토큰화 하는 과정에서 단어들은 숫자로 처리해 모델에서 학습 할 수 있도록 하는데, 이 과정에서 워드 임베딩(Word Embedding)이 사용됩니다.

워드 임베딩 방법에는 사전에 훈련된 워드 임베딩을 사용하기도 하고(GloVe, Word2Vec 등), 케라스에서 제공하는 Embedding Layer을 사용하기도 합니다.

임베딩 벡터를 생성하는 방법은 아래 그림과 같이 단순하게 볼 수 있습니다.

![image](https://user-images.githubusercontent.com/87166420/204735786-e9576dea-56e9-4c8f-b264-7ce039dfb787.png)


이런 수치화 과정을 거친 단어들은 LSTM셀에 시점 값의 형태로 입력됩니다.



LSTM셀에 대해 간략하게 설명하면, 

LSTM은 입출력을 시퀀스 단위로 처리하는 모델로, 가장 간단한 구조인 RNN의 Vanishing Gradient 현상을 해결하고자 발전된 모델입니다.

이때, 특징을 간단하게 본다면 LSTM셀에는 이전 시퀀스에서의 정보를 현재 시퀀스로 넘겨줄 수 있도록 메모리셀에 저장한다는 점 입니다.

이를 통해 이전 셀에서의 특징들이 많이 사라지지 않고, 현재에서 필요한 정보들을 더해서 다음 시점으로 넘겨 시퀀스가 길어지더라도 RNN보다 잊는게 덜해지는 것 입니다.

즉,  이러한 LSTM셀에 시퀀스 값으로 단어의 정보들이 들어가게되면 문장의 첫 단어부터 마지막 단어까지의 정보들을 종합해 넘어가기 때문에, 문장에 대한정보들이 요약된 context vector가 생성되었다고 할 수 있습니다.

---

### 2. 디코더

![image](https://user-images.githubusercontent.com/87166420/204735824-800a23bb-3b0f-4c39-bad2-64d4fabd4bfc.png)

디코더에서는 인코더에서 생성한 context vector을 가지고 출력 시퀀스의 각 단어별 확률값을 반환해 출력단어를 결정합니다.



디코더는 문장의 시작과 끝을 알리기 위해, 보통은 <sos>, <eos> 라는 토큰을 사용합니다.

디코더에서의 LSTM셀은 인코더에서 넘긴 context vector을 첫 번째 은닉 상태의 값으로 사용하게 되며, 학습과 테스트에서의 입력값이 조금 달라집니다.

우선 학습 과정에서는, 다음 값의 입력은 label에 쓰인 이전시점의 값이 됩니다.

즉, 위의 그림에서 '<sos> je suis etudiant' 라는 라벨값이 입력값으로 쓰이게 되는 겁니다.

즉, t시점에서 예측한 값 대신, t시점의 라벨값을 t+1시점의 입력값으로 쓰이는 것이고, 이러한 방식을 teaching forcing이라고 합니다.

이렇게 진행하는 이유는 학습 과정에서부터 이전 시점의 값을 사용하게 되면, 이전 시점의 값이 틀린 경우 제대로 학습이 되어지지 않기 때문에 비효율적일 수 있기 때문입니다.

테스트 과정, 즉 모델을 적용하는 과정에서의 입력값은 t시점의 예측값이 됩니다.

이렇게 각 시퀀스에서 출력되는 값들은 softmax 함수를 통해 가장 높은 확률의 단어를 결정하게 됩니다.

예측한 값과 실제 label 사이의 오차 계산을 통해 학습을 진행하기 위해 cross-entropy를 보통 사용합니다.

이 손실함수 결과가 역전파 되면서 가중치 행렬의 학습이 진행되고, 이 과정에서 임베딩 벡터까지 학습을 진행하기도 합니다.



---

>## Attention Mechanism

추가적으로 Sequence to Sequence를 적용할때 사용되는 메커니즘이 바로 attention mechanism 이라는게 있습니다.

Seq2Seq에서는 입력데이터가 길어지면, 아무리 RNN을 개선한 LSTM을 적용하게 되더라도 Vanishing Gradient를 완벽하게 해결할 수 없습니다.

따라서 Attetion이라는 걸 사용해 보완하게 되는데, 디코더에서 출력하는 단어를 예측하는 매 시점마다 인코더에서 예측단어와 연관있는 단어를 집중하는 방식입니다.

사실, Attention Mechanism 에는 다양한 종류가 있는데, 보통 수식적인 차이가 있고 방식에는 큰 변화는 없습니다.

과정은 다음과 같습니다.

1. 어텐션 스코어 구하기

   현재 디코더 시점에서의 단어 예측을 위해 인코더의 모든 은닉상태 값들이 현 디코더 시점의 값과 얼마나 유사한지를 판단합니다.

   유사한지 판단을 위해 내적을 보통 사용하게되고, 이외의 수식들을 사용하는 경우에 따라 Attention Mechanism 종류가 달라지게 됩니다.

   ![image](https://user-images.githubusercontent.com/87166420/204735865-60ab1eb8-425f-4622-8156-823b37b1e602.png)
   
2. 어텐션 분포 구하기

   attention 분포는 앞서 구한 어텐션 스코어를 Softmax를 적용해 분포를 구하게 되는 과정입니다.

   Softmax함수는 입력받은 값을 모두 0~1 사이의 값으로 정규화해 총합 1이 되도록 하기때문에 해당 값들에 대한 중요도로 볼 수 있습니다.

   ![image](https://user-images.githubusercontent.com/87166420/204735908-5a03d6d8-17d4-4718-9261-1f33e3ab811a.png)

3. 어텐션 값 구하기

   어텐션 최종 값을 얻기 위해 각 인코더의 은닉상태와 어텐션 가중치 값을 곱하고 최종적으로 모두 더하는 과정을 갖습니다.

   이 과정을 통해 현재 시점에서 예측해야하는 결과와 연관이 있는 부분들을 판단하게 됩니다.

   ![image](https://user-images.githubusercontent.com/87166420/204735923-82e1a38b-ee28-4253-bf4a-3829c9a0150a.png)

4. 최종 계산

   최종적인 계산은 앞선 모든 결과에 대해 디코딩 은닉상태와 결합해 하나의 벡터를 만들어 예측하는 과정입니다.

---

이번 포스팅에서는 seq2seq의 기본적인 구조, encoder과 decoder과정에서의 연산 과정들에 대해 알아보았습니다.

또한 seq2seq에서의 한계점인 vanishing gradient를 해결하기위한 attention mechanism에 대해서도 추가적으로 알아보았습니다.

----

> 참고 문헌
* https://velog.io/@dscwinterstudy/%EB%B0%91%EB%B0%94%EB%8B%A5%EB%B6%80%ED%84%B0-%EC%8B%9C%EC%9E%91%ED%95%98%EB%8A%94-%EB%94%A5%EB%9F%AC%EB%8B%9D2-7%EC%9E%A5.-RNN%EC%9D%84-%EC%82%AC%EC%9A%A9%ED%95%9C-%EB%AC%B8%EC%9E%A5-%EC%83%9D%EC%84%B1-5rk67hoiuv#72-seq2seq
* https://wikidocs.net/33793
* https://ratsgo.github.io/natural%20language%20processing/2017/03/09/rnnlstm/
* https://blog.naver.com/sooftware/221784472231
* https://wikidocs.net/22893
