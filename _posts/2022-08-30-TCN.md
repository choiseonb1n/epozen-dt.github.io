---
title: "TCN (Temporal Convolution Network)"
last_modified_at: 2022-8-30
author: 김상민
---

-------------

*이번 포스팅에서는 **TCN (Temporal Convolution Network)**에 관한 내용을 공유해보려고 합니다. 본 포스팅은 "An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling" 논문을 토대로 작성하였습니다.


---------------

> ## Abstract
   - 현재까지 대부분 딥러닝에서 sequence modeling은 RNN으로 인식
   - 최근 연구결과는 CNN으로 오디오, 기계 번역 등 sequence modeling task에서 RNN을 앞설 수 있다고 함
   - 이를 확인하기 위해 다양한 sequence modeling benchmark에서 CNN과 체계적인 비교 실험 진행
![abstract-tcn](https://user-images.githubusercontent.com/102953592/187330720-d84f1506-63b5-4971-afe0-28a0da4e66ed.JPG)


> ## TCN
   - ＂Temporal Convolutional Networks for Action Segmentation and Detection＂에서 제안한 모델
   - Encoder-Decoder TCN과 Dilated TCN 중 Dilated TCN을 사용, Causal Convolution을 사용
   - TCN = 1D FCN + causal convolutions
![tcn_structure](https://user-images.githubusercontent.com/102953592/187330920-6d3cffe9-867b-4aba-b033-fe5f3ec205cf.JPG)


> ## Dilated Convolution
![dilated convolution](https://user-images.githubusercontent.com/102953592/187335424-9f969005-7b93-4106-9d55-ea66fbd19ff3.JPG)
1) A stack of 1D convolution layer
2) A stack of 1D dilated convolution layer using 1, 2 and 4 dilation rate


> ## Causal Convolution
![causal convolution](https://user-images.githubusercontent.com/102953592/187335439-761ae1f1-5441-4a13-bef9-953a1e04bbe7.JPG)
1) Non-causal convolution
2) Causal convolution


> ## Advantage
  1) Parallelism
   - RNN Cell은 전 시점에 대하여 출력 값이 나온 후 다음 시점에 대한 출력 값 계산
   - TCN의 경우 전 시점에 대한 출력 값이 다음 시점에 필수요소가 아니기 때문에 구조적으로 병렬 처리 가능
  
  2) Flexible receptive field size
   - 수용필드 크기(dilation 크기, layer 개수, filter size 등)에 따라서 TCN의 메모리 크기를 유동적으로 제어 가능

  3) Stable gradients
   - 반복적인 RNN cell 구조와 달리 TCN은 이전 시점의 부분 그레디언트가 필요조건이 아니기 때문에 역 전파 구조가 다름
  
  4) Low memory requirement for training
   - RNN 계열 경우 각 시퀀스에 부부적인 게이트 결과물을 모두 저장하기 위하여 메모리 사용율이 올라감
   - TCN의 경우 컨볼루션 필터가 모두 공유되며 역전파 경로는 네트워크 깊이에만 의존적이므로 RNN 계열이 더 많은 곱셈 연산 필요

> ## Disadvantage
  1) Data storage during evaluation
   - 학습된 RNN 모델이 예측 할 때는 생각해보면 반복적으로 Cell에서 나오는 전 시점의 게이트 값들이 더 이전 시점들을 반영하는 값
   - 따라서 t시점의 예측을 위해서는 t-1시점의 게이트 값 이외에는 모두 메모리에서 휘발성으로 삭제할 수 있음
   - 반면에, TCN(컨볼루션) 계열은 모든 시퀀스에 대하여 결과값을 받아야함에 따라서 테스트 할 때 RNN처럼 시점 단위로 휘발성으로 사용하지 못하고 레이어 단위로만 휘발성으로 사용 가능
  
  2) Potential parameter change for a transfer of domain
   - 전이 학습(transfer learning)을 한다고 가정했을 때, 어떤 분야(A)에서는 작은 메모리 (작은 dilation 숫자, 작은 필터 사이즈)가 필요하지만 어떤 분야(B)에서는 큰 메모리 (큰 dilation 숫자, 큰 필터 사이즈)가 필요할 수 있음. 만약 (A->B)로 전이 학습을 한다고 가정 하였을 때 성능이 불량할 수 있음 (결국 receptive filed에 의존적으로 학습되기 때문)


> ## Experiment 
   - Evaluation of TCNs and recurrent architectures on synthetic stress tests
![Experiment](https://user-images.githubusercontent.com/102953592/187335691-b39c1cd7-9665-490d-98a7-5fb7fb91b3f6.JPG)


> ## The adding problem
![test1](https://user-images.githubusercontent.com/102953592/187335720-1ff35d21-e67f-4d34-af40-71c0bdb1743e.JPG)
   - 입력은 Tx2 이며 출력은 하나의 스칼라 값
   - 첫 번째 줄은 0~1 사이의 Random signal
   - 두 번째는 전체 T시점에서 2시점만 1로 두고 나머지는 0으로 설정
   - 출력 값은 1로 표현된 두 시점의 첫 번째 줄 값의 합
   - 해당 데이터는 LSTM 초창기 논문에서 제안
   - 제안한 알고리즘의 안정성을 측정하기 위한 데이터 셋


> ## Copy memory
![test2](https://user-images.githubusercontent.com/102953592/187335727-b50a3f0f-7eb4-4e06-b37b-aa53ed8bb406.JPG)
   - 길이는 T+20
   - 입력으로는 처음 10개에 1~8숫자 무작위 추출 후 마지막에서 11번째에 9를 입력 (복제 신호)
   - 출력으로는 9 다음 시점부터 뽑힌 무작위 숫자 10개를 맞춰야 함
   - 입력에서 시작 시점을 보여주었을 때 얼마나 잘 기억하는지를 확인하는 실험


> ## LAMBADA
![test3](https://user-images.githubusercontent.com/102953592/187335730-e2f1e819-3c7c-4ab0-a305-f0251612f9c8.JPG)
   - 소설에서 추출한 구절, 평균 4.6문장으로 단어 하나를 맞추는 것으로 구성된 데이터 셋
   - 훈련된 데이터 셋은 2억 개 이상이 단어를 가지고 2,662권의 소설
   - LAMBADA에서 성능이 잘 나오는 의미는 더 길고 넓은 영역에서 정보 추출을 잘한다는 의미


> ## Conclusion
   - 제안한 Causal Dilated Conv + Residual Block은 기본 LSTM, GRU, Vanilla RNN의 성능보다 우수
   - TCN이 RNN 계열보다 더 긴 메모리 보존성을 확인, RNN의 “무한한 기억(infinite memory)＂은 거의 없음을 확인
   - TCN이 sequence modeling에서 강력한 방법론으로 간주되어야 한다고 결론

이렇게 TCN과 관련되서 기본적인 구조와 다양한 데이터 실험을 통해 sequence modeling에서 성능을 알아보았습니다.

------------------------

> **참고 논문 및 자료**  
* [An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling](https://arxiv.org/pdf/1803.01271.pdf)
* [Youtude](https://www.youtube.com/watch?v=W6Cg0DQ9eTA)
