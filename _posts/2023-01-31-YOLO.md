---
title: "YOLO"
last_modified_at: 2023-01-27
author: 권재우
---

오늘은 Real-Time Object Detection 기술인 YOLO에 대해서 설명하겠습니다.


# YOLO
- 논문 주소  
https://arxiv.org/pdf/1506.02640.pdf


실제 논문은 위 링크에서 확인 가능합니다. 

![img4](https://user-images.githubusercontent.com/70212461/215013896-3e4c133e-4c96-4496-a3b9-a66185f62372.png)

YOLO는 "You Only Look Once"의 약자로 2016년에 발표 되었습니다.
기존의 Object Detection기법의 RCNN은 Classfication, Regression을 따로 예측하는 Two-Stage방법이었고, 이 방법은 시간이 오래 걸린다는 단점이 존재했습니다. 하지만 YOLO는 One-Stage방법으로 시간단축시키는 모델을 제안하면서 큰 인기를 얻었습니다.

YOLO가 나옴으로써 실시간으로 객체를 검출하는 모델의 성능이 크게 향상되었습니다.

## Architecture

![img2](https://user-images.githubusercontent.com/70212461/215013078-23d9b6e6-55d5-4fa1-b058-a0ad813c7af4.png)

모델 구조는 위 그림과 같습니다.

Yolo의 구조는 이미지 분류에 사용되는 GoogLeNet에서 따왔습니다. 총 24개의 Convolutional Layer와 2개의 Fully-Connected Layers로 구성되어 있습니다.

## Model

<img src="https://user-images.githubusercontent.com/70212461/215013547-ae501673-64f2-4542-80a1-87a02494433c.jpg" width="400" height="350">

모델의 최종 아웃풋은 클래스 확률(class probabilities)과 bounding box 위치정보(coordinates)입니다. bounding box의 위치정보에는 bounding box의 너비(width)와 높이(height)와 중심좌표(x, y)가 있습니다. 

하지만 YOLO학습을 위해서는 w,h,x,y를 모두 0~1 사이의 값으로 정규화(normalize)하는 작업이 필요합니다.

## 성능

<img src="https://user-images.githubusercontent.com/70212461/215014735-3da4b4a2-0520-4079-be42-4412890af3ac.png" width="350" height="300">

처음 나온 YOLO 모델의 성능은 기존 RCNN모델보다 정확도 부분에서는 조금 떨어지지만 속도부분에서 2~3배가량 빠른것으로 나타났습니다. 

현재는 YOLOv8까지 출시가 되었는데, 정확도부분도 RCNN 최근 모델과 비슷하다는 의견이 많기 때문에 Real-Time Object Detetion에서는 YOLO를 사용하는것을 추천합니다.   

이번 포스팅은 여기서 마무리하겠습니다 감사합니다.

### Reference

- https://arxiv.org/pdf/1506.02640.pdf

- https://bkshin.tistory.com/entry/%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0-YOLOYou-Only-Look-Once

- https://jerife.github.io/2021-10-29-yolov3/

- https://zeuskwon-ds.tistory.com/90